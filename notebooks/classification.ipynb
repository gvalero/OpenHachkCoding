{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['axes', 'boots', 'carabiners', 'crampons', 'gloves', 'hardshell_jackets', 'harnesses', 'helmets', 'insulated_jackets', 'pulleys', 'rope', 'tents']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(data_dir)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "img_width, img_height = 128, 128\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for label in labels:\n",
    "    files = (os.listdir(os.path.join(data_dir,label)))\n",
    "    for file in files:\n",
    "        img = Image.open(os.path.join(data_dir, label,file)).convert('RGB')\n",
    "        width, height = img.size\n",
    "        if height> width:\n",
    "            ratio = float(img_height/height)\n",
    "        else:\n",
    "            ratio = float(img_width/width)\n",
    "        new_size = int(width*ratio), int(height*ratio)\n",
    "        img = img.resize(new_size, Image.ANTIALIAS)\n",
    "        delta_w = img_width - img.size[0]\n",
    "        delta_h = img_height - img.size[1]\n",
    "        padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "        img = ImageOps.expand(img, padding, fill='white')\n",
    "        data = np.asarray(img)\n",
    "        X.append(data)\n",
    "        y.append(labels.index(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "c = list(zip(X, y))\n",
    "random.shuffle(c)\n",
    "X, y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3277128, 1: 2603457, 2: 8583803, 3: 3841970, 4: 4092267, 5: 9818918, 6: 4174152, 7: 2374193, 8: 4531202, 9: 928519, 10: 5585056, 11: 3703631}\n"
     ]
    }
   ],
   "source": [
    "colors = {}\n",
    "for idx, x in enumerate(X):\n",
    "    unique, counts = np.unique(x, return_counts=True)\n",
    "    dict_ = (dict(zip(unique, counts)))\n",
    "    if y[idx] not in colors.keys():\n",
    "        colors[y[idx]] = 0\n",
    "    if 255 in dict_.keys():\n",
    "        colors[y[idx]]+=(dict_[255])\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['axes', 'boots', 'carabiners', 'crampons', 'gloves', 'hardshell_jackets', 'harnesses', 'helmets', 'insulated_jackets', 'pulleys', 'rope', 'tents']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEJCAYAAABv6GdPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEUtJREFUeJzt3W2MpWV9x/Hvz2VFG6gYdxoJ7Dq2oi0SBZwg1sRSsA2CgRdCs6Q+YLGbEvGhNW3AJljpG21TaSxWuhYiWAtYNHZFqMECARtBB1wQWGm3SmUDCSsPiwTBLv774hztcDiz5z4zZ+bAxfeTnHA/XOe+/1zM/vbimvshVYUkqS3Pm3YBkqTJM9wlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDdprWidet25dzc7OTuv0kvSsdPPNN/+oqmZGtZtauM/OzjI/Pz+t00vSs1KS/+nSzmkZSWqQ4S5JDTLcJalBhrskNchwl6QGdQ73JGuSfCfJFUP27Z3ksiTbk9yUZHaSRUqSxjPOyP0DwLZF9p0GPFRVrwDOBT6+3MIkSUvXKdyTHAgcD/zjIk1OBC7qL18OHJMkyy9PkrQUXUfufwv8GfCzRfYfANwDUFW7gV3AS5ZdnSRpSUbeoZrkrcD9VXVzkqMWazZk29PevJ1kE7AJYMOGDWOUqWmaPfOrUznv3R87firnlVrQZeT+RuCEJHcDlwJHJ/mngTY7gPUASfYCXgQ8OHigqtpcVXNVNTczM/LRCJKkJRoZ7lV1VlUdWFWzwEbgmqp6+0CzLcC7+ssn9ds8beQuSVodS35wWJJzgPmq2gJcAHwuyXZ6I/aNE6pPkrQEY4V7VV0HXNdfPnvB9seBkydZmCRp6bxDVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkho0MtyTvCDJt5LcmuSOJB8d0ubUJDuTbO1/3rMy5UqSuujymr0ngKOr6tEka4FvJLmqqm4caHdZVZ0x+RIlSeMaGe5VVcCj/dW1/U+tZFGSpOXpNOeeZE2SrcD9wNVVddOQZm9LcluSy5Osn2iVkqSxdAr3qnqyqg4FDgSOSHLIQJOvALNV9Rrg68BFw46TZFOS+STzO3fuXE7dkqQ9GOtqmap6GLgOOHZg+wNV9UR/9TPA6xb5/uaqmququZmZmSWUK0nqosvVMjNJ9usvvxB4M/C9gTb7L1g9Adg2ySIlSePpcrXM/sBFSdbQ+8vgC1V1RZJzgPmq2gK8P8kJwG7gQeDUlSpYkjRal6tlbgMOG7L97AXLZwFnTbY0SdJSeYeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNajLO1RfkORbSW5NckeSjw5ps3eSy5JsT3JTktmVKFaS1E2XkfsTwNFV9VrgUODYJEcOtDkNeKiqXgGcC3x8smVKksYxMtyr59H+6tr+pwaanQhc1F++HDgmSSZWpSRpLJ3m3JOsSbIVuB+4uqpuGmhyAHAPQFXtBnYBLxlynE1J5pPM79y5c3mVS5IW1Sncq+rJqjoUOBA4IskhA02GjdIHR/dU1eaqmququZmZmfGrlSR1MtbVMlX1MHAdcOzArh3AeoAkewEvAh6cQH2SpCXocrXMTJL9+ssvBN4MfG+g2RbgXf3lk4BrquppI3dJ0urYq0Ob/YGLkqyh95fBF6rqiiTnAPNVtQW4APhcku30RuwbV6xiSdJII8O9qm4DDhuy/ewFy48DJ0+2NEnSUnmHqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWoyztU1ye5Nsm2JHck+cCQNkcl2ZVka/9z9rBjSZJWR5d3qO4GPlRVtyTZF7g5ydVVdedAuxuq6q2TL1GSNK6RI/equq+qbukv/xjYBhyw0oVJkpZurDn3JLP0XpZ905Ddb0hya5Krkrx6ke9vSjKfZH7nzp1jFytJ6qZzuCfZB/gi8MGqemRg9y3Ay6rqtcDfAV8edoyq2lxVc1U1NzMzs9SaJUkjdAr3JGvpBfvnq+pLg/ur6pGqerS/fCWwNsm6iVYqSeqsy9UyAS4AtlXVJxZp89J+O5Ic0T/uA5MsVJLUXZerZd4IvAP4bpKt/W0fBjYAVNX5wEnA6Ul2Az8BNlZVrUC9kqQORoZ7VX0DyIg25wHnTaooSdLyeIeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNajLO1TXJ7k2ybYkdyT5wJA2SfLJJNuT3Jbk8JUpV5LURZd3qO4GPlRVtyTZF7g5ydVVdeeCNm8BDup/Xg98uv9PSdIUjBy5V9V9VXVLf/nHwDbggIFmJwIXV8+NwH5J9p94tZKkTrqM3H8hySxwGHDTwK4DgHsWrO/ob7tv4PubgE0AGzZsGK/SZ4jZM786lfPe/bHjp3JeSc9OnX+hmmQf4IvAB6vqkcHdQ75ST9tQtbmq5qpqbmZmZrxKJUmddQr3JGvpBfvnq+pLQ5rsANYvWD8QuHf55UmSlqLL1TIBLgC2VdUnFmm2BXhn/6qZI4FdVXXfIm0lSSusy5z7G4F3AN9NsrW/7cPABoCqOh+4EjgO2A48Brx78qVKkroaGe5V9Q2Gz6kvbFPAeydVlCRpebxDVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoC6v2bswyf1Jbl9k/1FJdiXZ2v+cPfkyJUnj6PKavc8C5wEX76HNDVX11olUJElatpEj96q6HnhwFWqRJE3IpObc35Dk1iRXJXn1hI4pSVqiLtMyo9wCvKyqHk1yHPBl4KBhDZNsAjYBbNiwYQKnliQNs+yRe1U9UlWP9pevBNYmWbdI281VNVdVczMzM8s9tSRpEcsO9yQvTZL+8hH9Yz6w3ONKkpZu5LRMkkuAo4B1SXYAHwHWAlTV+cBJwOlJdgM/ATZWVa1YxZKkkUaGe1WdMmL/efQulZQkPUN4h6okNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aGS4J7kwyf1Jbl9kf5J8Msn2JLclOXzyZUqSxtFl5P5Z4Ng97H8LcFD/swn49PLLkiQtx8hwr6rrgQf30ORE4OLquRHYL8n+kypQkjS+Scy5HwDcs2B9R3+bJGlK9prAMTJkWw1tmGyiN3XDhg0blnzC2TO/uuTvPlv57/zccPfHjp92CavO/84rYxIj9x3A+gXrBwL3DmtYVZuraq6q5mZmZiZwaknSMJMI9y3AO/tXzRwJ7Kqq+yZwXEnSEo2clklyCXAUsC7JDuAjwFqAqjofuBI4DtgOPAa8e6WKlSR1MzLcq+qUEfsLeO/EKpIkLZt3qEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDOoV7kmOT3JVke5Izh+w/NcnOJFv7n/dMvlRJUldd3qG6BvgU8DvADuDbSbZU1Z0DTS+rqjNWoEZJ0pi6jNyPALZX1fer6qfApcCJK1uWJGk5uoT7AcA9C9Z39LcNeluS25JcnmT9RKqTJC1Jl3DPkG01sP4VYLaqXgN8Hbho6IGSTUnmk8zv3LlzvEolSZ11CfcdwMKR+IHAvQsbVNUDVfVEf/UzwOuGHaiqNlfVXFXNzczMLKVeSVIHXcL928BBSV6e5PnARmDLwgZJ9l+wegKwbXIlSpLGNfJqmaraneQM4GvAGuDCqrojyTnAfFVtAd6f5ARgN/AgcOoK1ixJGmFkuANU1ZXAlQPbzl6wfBZw1mRLkyQtlXeoSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoM6hXuSY5PclWR7kjOH7N87yWX9/TclmZ10oZKk7kaGe5I1wKeAtwAHA6ckOXig2WnAQ1X1CuBc4OOTLlSS1F2XkfsRwPaq+n5V/RS4FDhxoM2JwEX95cuBY5JkcmVKksbRJdwPAO5ZsL6jv21om6raDewCXjKJAiVJ49urQ5thI/BaQhuSbAI29VcfTXJXh/M/W6wDfjTtIp5h7JOnGtkfee5NaD4nf0b28N+5S3+8rMs5uoT7DmD9gvUDgXsXabMjyV7Ai4AHBw9UVZuBzV0Ke7ZJMl9Vc9Ou45nEPnkq++Pp7JOnmmR/dJmW+TZwUJKXJ3k+sBHYMtBmC/Cu/vJJwDVV9bSRuyRpdYwcuVfV7iRnAF8D1gAXVtUdSc4B5qtqC3AB8Lkk2+mN2DeuZNGSpD3rMi1DVV0JXDmw7ewFy48DJ0+2tGedJqeblsk+eSr74+nsk6eaWH/E2RNJao+PH5CkBhnuY+rwKIY/SXJnktuS/HuSTpctPVuN6o8F7U5KUkmavzKiS58k+b3+z8kdSf55tWtcTR3+zGxIcm2S7/T/3Bw3jTpXS5ILk9yf5PZF9ifJJ/v9dVuSw5d0oqry0/FD7xfK/w38KvB84Fbg4IE2vw38Un/5dOCyadc9zf7ot9sXuB64EZibdt3T7hPgIOA7wIv7678y7bqn3B+bgdP7ywcDd0+77hXukzcBhwO3L7L/OOAqevcPHQnctJTzOHIfz8hHMVTVtVX1WH/1Rnr3BbSqy6MpAP4S+Cvg8dUsbkq69MkfAp+qqocAqur+Va5xNXXpjwJ+ub/8Ip5+H01Tqup6htwHtMCJwMXVcyOwX5L9xz2P4T6eLo9iWOg0en8Dt2pkfyQ5DFhfVVesZmFT1OVn5JXAK5P8R5Ibkxy7atWtvi798RfA25PsoHdV3vtWp7RnrHFzZqhOl0LqFzo9ZgEgyduBOeC3VrSi6dpjfyR5Hr2nhJ66WgU9A3T5GdmL3tTMUfT+z+6GJIdU1cMrXNs0dOmPU4DPVtXfJHkDvXtmDqmqn618ec9InXNmTxy5j6fLoxhI8mbgz4ETquqJVaptGkb1x77AIcB1Se6mN3+4pfFfqnZ9XMe/VtX/VtUPgLvohX2LuvTHacAXAKrqm8AL6D1j5bmqU86MYriPZ+SjGPrTEP9AL9hbnkuFEf1RVbuqal1VzVbVLL3fQZxQVfPTKXdVdHlcx5fp/eKdJOvoTdN8f1WrXD1d+uOHwDEASX6DXrjvXNUqn1m2AO/sXzVzJLCrqu4b9yBOy4yhuj2K4a+BfYB/6T/S/odVdcLUil5BHfvjOaVjn3wN+N0kdwJPAn9aVQ9Mr+qV07E/PgR8Jskf05t+OLX6l420KMkl9Kbk1vV/z/ARYC1AVZ1P7/cOxwHbgceAdy/pPA33oSQ9ZzktI0kNMtwlqUGGuyQ1yHCXpAYZ7pK0CkY9MGyg7blJtvY//5lk7BvcvFpGklZBkjcBj9J7bswhY3zvfcBhVfUH45zPkbskrYJhDwxL8mtJ/i3JzUluSPLrQ756CnDJuOfzJiZJmp7NwB9V1X8leT3w98DRP9/Zfx/Ey4Frxj2w4S5JU5BkH+A3+f+72QH2Hmi2Ebi8qp4c9/iGuyRNx/OAh6vq0D202Qi8d6kHlyStsqp6BPhBkpPhF6/Xe+3P9yd5FfBi4JtLOb7hLkmroP/AsG8Cr0qyI8lpwO8DpyW5FbiDp76l6hTg0qU+RM1LISWpQY7cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ36PzOBuqPQKt0/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(colors.values())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahulsta\\appdata\\local\\continuum\\anaconda3\\envs\\openhack\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1697 samples, validate on 425 samples\n",
      "Epoch 1/10\n",
      "1697/1697 [==============================] - 14s 8ms/step - loss: 0.0055 - acc: 0.5651 - val_loss: 0.0023 - val_acc: 0.8235\n",
      "Epoch 2/10\n",
      "1697/1697 [==============================] - 14s 8ms/step - loss: 0.0023 - acc: 0.8079 - val_loss: 0.0020 - val_acc: 0.8612\n",
      "Epoch 3/10\n",
      "1697/1697 [==============================] - 14s 8ms/step - loss: 0.0016 - acc: 0.8674 - val_loss: 0.0013 - val_acc: 0.9059\n",
      "Epoch 4/10\n",
      "1697/1697 [==============================] - 15s 9ms/step - loss: 8.5463e-04 - acc: 0.9287 - val_loss: 0.0015 - val_acc: 0.9035\n",
      "Epoch 5/10\n",
      "1697/1697 [==============================] - 16s 10ms/step - loss: 6.9606e-04 - acc: 0.9381 - val_loss: 0.0015 - val_acc: 0.9224\n",
      "Epoch 6/10\n",
      "1697/1697 [==============================] - 16s 9ms/step - loss: 5.8550e-04 - acc: 0.9523 - val_loss: 0.0012 - val_acc: 0.9200\n",
      "Epoch 7/10\n",
      "1697/1697 [==============================] - 16s 10ms/step - loss: 4.9479e-04 - acc: 0.9635 - val_loss: 0.0011 - val_acc: 0.9318\n",
      "Epoch 8/10\n",
      "1697/1697 [==============================] - 17s 10ms/step - loss: 4.8552e-04 - acc: 0.9629 - val_loss: 0.0011 - val_acc: 0.9412\n",
      "Epoch 9/10\n",
      "1697/1697 [==============================] - 16s 9ms/step - loss: 3.4899e-04 - acc: 0.9705 - val_loss: 0.0013 - val_acc: 0.9294\n",
      "Epoch 10/10\n",
      "1697/1697 [==============================] - 16s 10ms/step - loss: 2.3550e-04 - acc: 0.9782 - val_loss: 0.0013 - val_acc: 0.9271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22411d485c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import random\n",
    "\n",
    "# dimensions of our images.\n",
    "num_classes=len(labels)\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)\n",
    "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 3)\n",
    "input_shape = (img_width, img_height, 3)\n",
    "X\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(X_train)/255, np.array(y_train)/255, validation_split=0.2, nb_epoch=10, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahulsta\\appdata\\local\\continuum\\anaconda3\\envs\\openhack\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "c:\\users\\lahulsta\\appdata\\local\\continuum\\anaconda3\\envs\\openhack\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., epochs=25)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "425/425 [==============================] - 26s 61ms/step - loss: 1.3047 - acc: 0.6100 - val_loss: 1.1946 - val_acc: 0.8118\n",
      "Epoch 2/25\n",
      "425/425 [==============================] - 19s 46ms/step - loss: 0.9267 - acc: 0.7124 - val_loss: 1.0934 - val_acc: 0.8047\n",
      "Epoch 3/25\n",
      "425/425 [==============================] - 17s 39ms/step - loss: 0.8746 - acc: 0.7441 - val_loss: 1.2226 - val_acc: 0.8518\n",
      "Epoch 4/25\n",
      "425/425 [==============================] - 21s 49ms/step - loss: 0.7522 - acc: 0.7782 - val_loss: 1.1822 - val_acc: 0.8565\n",
      "Epoch 5/25\n",
      "425/425 [==============================] - 21s 49ms/step - loss: 0.7623 - acc: 0.7747 - val_loss: 0.9256 - val_acc: 0.8776\n",
      "Epoch 6/25\n",
      "425/425 [==============================] - 20s 47ms/step - loss: 0.7087 - acc: 0.8024 - val_loss: 1.0988 - val_acc: 0.8471\n",
      "Epoch 7/25\n",
      "425/425 [==============================] - 23s 54ms/step - loss: 0.8059 - acc: 0.7800 - val_loss: 1.0667 - val_acc: 0.8541\n",
      "Epoch 8/25\n",
      "425/425 [==============================] - 20s 46ms/step - loss: 0.7224 - acc: 0.8006 - val_loss: 0.9221 - val_acc: 0.8824\n",
      "Epoch 9/25\n",
      "425/425 [==============================] - 18s 42ms/step - loss: 0.7185 - acc: 0.7912 - val_loss: 0.9613 - val_acc: 0.8918\n",
      "Epoch 10/25\n",
      "425/425 [==============================] - 24s 57ms/step - loss: 0.7337 - acc: 0.8006 - val_loss: 1.2175 - val_acc: 0.8565\n",
      "Epoch 11/25\n",
      "425/425 [==============================] - 19s 44ms/step - loss: 0.7124 - acc: 0.8141 - val_loss: 0.9762 - val_acc: 0.8706\n",
      "Epoch 12/25\n",
      "425/425 [==============================] - 18s 43ms/step - loss: 0.6484 - acc: 0.8282 - val_loss: 0.9969 - val_acc: 0.8800\n",
      "Epoch 13/25\n",
      "425/425 [==============================] - 21s 50ms/step - loss: 0.7220 - acc: 0.8112 - val_loss: 1.2041 - val_acc: 0.8635\n",
      "Epoch 14/25\n",
      "425/425 [==============================] - 20s 47ms/step - loss: 0.6522 - acc: 0.8259 - val_loss: 1.3571 - val_acc: 0.8635\n",
      "Epoch 15/25\n",
      "425/425 [==============================] - 21s 49ms/step - loss: 0.7242 - acc: 0.8147 - val_loss: 1.3212 - val_acc: 0.8565\n",
      "Epoch 16/25\n",
      "425/425 [==============================] - 21s 49ms/step - loss: 0.6616 - acc: 0.8224 - val_loss: 1.0498 - val_acc: 0.8776\n",
      "Epoch 17/25\n",
      "425/425 [==============================] - 22s 51ms/step - loss: 0.6814 - acc: 0.8288 - val_loss: 0.9461 - val_acc: 0.8965\n",
      "Epoch 18/25\n",
      "425/425 [==============================] - 26s 60ms/step - loss: 0.7247 - acc: 0.8206 - val_loss: 1.0498 - val_acc: 0.8659\n",
      "Epoch 19/25\n",
      "425/425 [==============================] - 27s 64ms/step - loss: 0.7020 - acc: 0.8265 - val_loss: 0.9746 - val_acc: 0.8871\n",
      "Epoch 20/25\n",
      "425/425 [==============================] - 28s 67ms/step - loss: 0.7066 - acc: 0.8229 - val_loss: 1.3051 - val_acc: 0.8306\n",
      "Epoch 21/25\n",
      "425/425 [==============================] - 27s 65ms/step - loss: 0.6914 - acc: 0.8212 - val_loss: 0.9771 - val_acc: 0.8776\n",
      "Epoch 22/25\n",
      "425/425 [==============================] - 27s 64ms/step - loss: 0.6604 - acc: 0.8382 - val_loss: 1.0771 - val_acc: 0.8871\n",
      "Epoch 23/25\n",
      "425/425 [==============================] - 24s 56ms/step - loss: 0.6068 - acc: 0.8412 - val_loss: 1.0764 - val_acc: 0.8871\n",
      "Epoch 24/25\n",
      "425/425 [==============================] - 23s 54ms/step - loss: 0.6821 - acc: 0.8429 - val_loss: 1.1214 - val_acc: 0.8871\n",
      "Epoch 25/25\n",
      "425/425 [==============================] - 20s 47ms/step - loss: 0.6670 - acc: 0.8429 - val_loss: 0.9213 - val_acc: 0.9153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2250ead5550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "X_train_, X_val = X_train[0:int(len(X_train)*0.8)], X_train[int(len(X_train)*0.8):]\n",
    "y_train_, y_val = y_train[0:int(len(y_train)*0.8)], y_train[int(len(y_train)*0.8):]\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function = preprocess_input)\n",
    "\n",
    "datagen_val = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function = preprocess_input)\n",
    "\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen_train.fit(X_train_)\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen_val.fit(X_val)\n",
    "\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen_train.flow(X_train_, y_train_, batch_size=batch_size), validation_data=datagen_val.flow(X_val, y_val, batch_size=batch_size),nb_epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
